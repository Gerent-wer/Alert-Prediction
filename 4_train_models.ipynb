{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ab26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. import necessary libraries and modules\n",
    "\n",
    "#2. upload all necessary data\n",
    "\n",
    "#3. define X and y. y - our target. X- predictors. We have X but how we represent y? -  Vlad say that for \n",
    "#   now we do not devide our dataset into two this parts, but I suppose you should ask Markiyan about\n",
    "#   how we can measure the efficiency of the model later on\n",
    "\n",
    "#4. devide data into train and test using following link\n",
    "    #use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n",
    "\n",
    "#5. train 6 models:\n",
    "    # 1. LogisticRegression\n",
    "    # 2. Support Vector Machines\n",
    "    # 3. Stochastic Gradient Descent\n",
    "    # 4. Random Forest\n",
    "    # 5. Nearest Neighbors Classification\n",
    "    # 6. Bayesian Regression\n",
    "    \n",
    "# find out how to import and use them here: https://scikit-learn.org/stable/supervised_learning.html\n",
    "    \n",
    "#6. save them\n",
    "\n",
    "#7 Calculate confusion matrices for each model\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matri x.html\n",
    "\n",
    "#8 Choose the top 3 models. Motivate your decision on how did you define “top” models?\n",
    "\n",
    "#9 Tune hyperparameters for the top 3 models you choose\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "#10. move to the next file - 5_evaluate_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12772d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ae315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikita_voitishyn/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (25,44,45,47,48,49,51,52,55,57,58,59,62) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv('data/final_dataset_ready_to_train.csv')\n",
    "\n",
    "\n",
    "#USE this if firt row crashes\n",
    "\n",
    "#weather_alarms = pd.read_csv('data/df_weather+alarms_merged.csv')\n",
    "#tfidf_vectors = pd.read_csv('data/0_isw_data_collection/3_isw_vectorised_with_date.csv')\n",
    "#tfidf_vectors = tfidf_vectors.drop(columns=['Unnamed: 0'],axis=1)\n",
    "#weather_alarms = weather_alarms.drop(columns=['Unnamed: 0'],axis=1)\n",
    "#weather_alarms = weather_alarms.drop(columns=['city_resolvedAddress'],axis=1)\n",
    "#weather_alarms = weather_alarms.rename(columns={\"day_datetime\": \"date\"})\n",
    "#merged_df = pd.merge(weather_alarms, tfidf_vectors, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb6ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test parts\n",
    "\n",
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "\n",
    "#...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
